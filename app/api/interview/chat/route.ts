// @ts-nocheck
import { NextResponse } from 'next/server';
import Groq from 'groq-sdk';
import OpenAI from 'openai';
import { createClient } from '@supabase/supabase-js';

// å„ç¨®ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®æº–å‚™
const groq = new Groq({ apiKey: process.env.GROQ_API_KEY });
const openai = new OpenAI({ apiKey: process.env.OPENAI_API_KEY });
const supabase = createClient(
  process.env.NEXT_PUBLIC_SUPABASE_URL!,
  process.env.NEXT_PUBLIC_SUPABASE_ANON_KEY!
);

export async function POST(req: Request) {
  const RESPONSE_TIME_SECONDS = 15; // AIã®å¿œç­”ã«ã‹ã‹ã£ãŸæ™‚é–“ï¼ˆåˆ©ç”¨æ™‚é–“ã¨ã—ã¦åŠ ç®—ï¼‰
  
  // letå®£è¨€ã§åˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼ã‚’å›é¿
  let userText = "èãå–ã‚Šå¤±æ•—"; 
  let aiText = "ã‚¨ãƒ©ãƒ¼";
  const dummyId = "00000000-0000-0000-0000-000000000000"; // ä»®ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ID

  if (!process.env.NEXT_PUBLIC_SUPABASE_URL) {
    console.error("âŒ Supabaseã®éµãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ï¼.env.localã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚");
    return NextResponse.json({ error: "Env vars missing" }, { status: 500 });
  }

  try {
    const formData = await req.formData();
    const audioFile = formData.get('audio') as File;
    
    // --- ã€åˆ©ç”¨åˆ¶é™ãƒã‚§ãƒƒã‚¯ã€‘ ---
    const { data: usageData } = await supabase
        .from("daily_usages")
        .select("total_seconds")
        .eq("user_id", dummyId)
        .eq("usage_date", new Date().toISOString().split('T')[0])
        .maybeSingle();

    const currentSeconds = usageData?.total_seconds || 0;
    const remainingSeconds = 900 - currentSeconds;

    if (remainingSeconds <= 0) {
        const message = "ç”³ã—è¨³ã‚ã‚Šã¾ã›ã‚“ã€‚æœ¬æ—¥ã®åˆ©ç”¨æ™‚é–“ã‚’ä½¿ã„åˆ‡ã‚Šã¾ã—ãŸï¼ˆ15åˆ†ï¼‰ã€‚ç¶šãã¯æ˜æ—¥ã¾ãŸãŠè¶Šã—ãã ã•ã„ã€‚";
        const mp3Response = await openai.audio.speech.create({
            model: "tts-1", voice: "alloy", input: message,
        });
        const audioData = await mp3Response.arrayBuffer();
        
        return new NextResponse(audioData, {
            headers: { 
                'Content-Type': 'audio/mpeg', 
                'x-ai-text': encodeURIComponent(message),
                'x-remaining-seconds': '0'
            },
        });
    }

    // --- ã€è€³ã€‘Groqã§æ–‡å­—èµ·ã“ã— ---
    if (audioFile) {
        const buffer = await audioFile.arrayBuffer();
        const transcription = await groq.audio.transcriptions.create({
            file: new File([buffer], 'input.wav', { type: 'audio/wav' }),
            model: 'whisper-large-v3', language: 'ja', response_format: 'json',
        });
        userText = transcription.text;
    }
    
    // --- ã€è„³ã€‘GPT-4o miniã§è¿”ç­”ç”Ÿæˆ ---
    const completion = await openai.chat.completions.create({
        model: "gpt-4o-mini",
        messages: [
            { role: "system", content: "ã‚ãªãŸã¯å„ªã—ã„é¢æ¥å®˜ã§ã™ã€‚100æ–‡å­—ä»¥å†…ã§ã€ç›¸æ§Œã‚’æ‰“ã¡ãªãŒã‚‰æ¬¡ã®è³ªå•ã‚’ã—ã¦ãã ã•ã„ã€‚" },
            { role: "user", content: userText }
        ],
    });
    aiText = completion.choices[0].message.content || "ã‚¨ãƒ©ãƒ¼";

    // --- ã€è¨˜æ†¶ã€‘Supabaseã«ä¿å­˜ã¨åˆ©ç”¨æ™‚é–“æ›´æ–° ---
    const newTotalSeconds = currentSeconds + RESPONSE_TIME_SECONDS;

    // 1. åˆ©ç”¨æ™‚é–“ã®æ›´æ–°ï¼ˆã‚¢ãƒƒãƒ—ã‚µãƒ¼ãƒˆï¼‰
    await supabase.from("daily_usages").upsert({
        user_id: dummyId,
        usage_date: new Date().toISOString().split('T')[0],
        total_seconds: newTotalSeconds,
    });
    
    // 2. ä¼šè©±ãƒ­ã‚°ã®ä¿å­˜ï¼ˆé¢æ¥ãƒ­ã‚°ï¼‰
    await supabase.from("interviews").insert({
        user_id: dummyId,
        transcript: userText,
        feedback: aiText,
        score: 80 
    });

    // --- ã€å£ã€‘OpenAI TTSã§éŸ³å£°åˆæˆ ---
    const mp3Response = await openai.audio.speech.create({
      model: "tts-1",
      voice: "alloy",
      input: aiText,
    });
    
    const audioData = await mp3Response.arrayBuffer();
    
    return new NextResponse(audioData, {
      headers: {
        'Content-Type': 'audio/mpeg',
        'x-ai-text': encodeURIComponent(aiText),
        'x-remaining-seconds': String(900 - newTotalSeconds) // æ®‹ã‚Šæ™‚é–“ã‚’è¿”ã™
      },
    });

  } catch (error) {
    console.error("ğŸ’¥ è‡´å‘½çš„ãªã‚¨ãƒ©ãƒ¼:", error);
    // ã‚¨ãƒ©ãƒ¼æ™‚ã§ã‚‚ã‚¢ãƒ—ãƒªãŒå£Šã‚Œãªã„ã‚ˆã†ã€ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’éŸ³å£°ã§è¿”ã™
    const message = "ç”³ã—è¨³ã‚ã‚Šã¾ã›ã‚“ã€‚ã‚·ã‚¹ãƒ†ãƒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚";
    const mp3Response = await openai.audio.speech.create({
        model: "tts-1", voice: "alloy", input: message,
    });
    const audioData = await mp3Response.arrayBuffer();

    return new NextResponse(audioData, {
        headers: { 'Content-Type': 'audio/mpeg', 'x-ai-text': encodeURIComponent("ã‚·ã‚¹ãƒ†ãƒ ã‚¨ãƒ©ãƒ¼"), 'x-remaining-seconds': '900' },
    });
  }
}